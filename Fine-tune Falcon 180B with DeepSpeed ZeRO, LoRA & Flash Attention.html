<!DOCTYPE html>
<!-- saved from url=(0056)https://www.philschmid.de/deepspeed-lora-flash-attention -->
<html lang="en" class="scroll-smooth light" coupert-item="9AF8D9A4E502F3784AD24272D81F0381" style="color-scheme: light;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script type="text/javascript" async="" src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/analytics.js.tÃ©lÃ©chargement"></script><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if("system"===e||(!e&&true)){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else if(e) d.add(e)}catch(e){}}()</script><meta content="width=device-width, initial-scale=1" name="viewport"><title>Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention</title><meta name="robots" content="follow, index"><meta name="description" content="In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine."><meta property="og:url" content="https://www.philschmid.de/deepspeed-lora-flash-attention"><meta property="og:type" content="article"><meta property="og:site_name" content="philschmid blog"><meta property="og:description" content="In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine."><meta property="og:title" content="Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention"><meta property="og:image" content="https://www.philschmid.de/static/blog/deepspeed-lora-flash-attention/thumbnail.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="https://twitter.com/_philschmid"><meta name="twitter:title" content="Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention"><meta name="twitter:description" content="In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine."><meta name="twitter:image" content="https://www.philschmid.de/static/blog/deepspeed-lora-flash-attention/thumbnail.jpg"><link rel="canonical" href="https://www.philschmid.de/deepspeed-lora-flash-attention"><meta property="article:published_time" content="2023-09-20T00:00:00.000Z"><meta property="article:modified_time" content="2023-09-20T00:00:00.000Z"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.philschmid.de/deepspeed-lora-flash-attention"
  },
  "headline": "Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA & Flash Attention",
  "image": [
    {
      "@type": "ImageObject",
      "url": "https://www.philschmid.de/static/blog/deepspeed-lora-flash-attention/thumbnail.jpg"
    }
  ],
  "datePublished": "2023-09-20T00:00:00.000Z",
  "dateModified": "2023-09-20T00:00:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "Philipp Schmid"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "Philipp Schmid",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.philschmid.de/static/images/logo.png"
    }
  },
  "description": "In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine."
}</script><meta name="next-head-count" content="21"><link rel="apple-touch-icon" sizes="76x76" href="https://www.philschmid.de/static/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://www.philschmid.de/static/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://www.philschmid.de/static/favicons/favicon-16x16.png"><link rel="manifest" href="https://www.philschmid.de/static/favicons/site.webmanifest"><link rel="mask-icon" href="https://www.philschmid.de/static/favicons/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#000000"><meta name="theme-color" content="#000000"><link rel="alternate" type="application/rss+xml" href="https://www.philschmid.de/feed.xml"><link rel="preload" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/b4344d453e1745b0.css" as="style"><link rel="stylesheet" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/b4344d453e1745b0.css" data-n-g=""><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/polyfills-5cd94c89d3acac5f.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/webpack-7648af1b1f5d8cbf.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/main-619387dc95538b71.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/_app-fb46e531cb10d1cb.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/675-64a821d6ab4fd557.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/410-1fac91fa0a8ba801.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/219-e2a5c17e19ee2d74.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/712-24352cc5737433fc.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/[...slug]-d5ef88106c16d5ab.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/_buildManifest.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/_ssgManifest.js.tÃ©lÃ©chargement" defer=""></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/_middlewareManifest.js.tÃ©lÃ©chargement" defer=""></script><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/index-6353731beeb76ac8.js.tÃ©lÃ©chargement"><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/cloud-attention-1239786830d628f3.js.tÃ©lÃ©chargement"><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/tags-6899b210840c7080.js.tÃ©lÃ©chargement"><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/projects-6b079a3b27a4ee68.js.tÃ©lÃ©chargement"><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/philipp-schmid-5fe320197331417d.js.tÃ©lÃ©chargement"><link as="script" rel="prefetch" href="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/[tag]-2deb40458c121ea8.js.tÃ©lÃ©chargement"></head><body class="bg-white text-black antialiased dark:bg-gray-900 dark:text-white"><div id="__next" data-reactroot=""><div class="mx-auto max-w-3xl px-1.5 sm:px-6 lg:max-w-4xl xl:max-w-6xl xl:px-0"><div class="flex h-screen flex-col justify-between"><header class="flex items-center justify-between py-2 md:py-10"><div><a aria-label="philschmid blog" href="https://www.philschmid.de/"><div class="flex items-center justify-between"><div class="mr-3"><svg class="fill-black dark:fill-gray-800" width="50" height="50" viewBox="0 0 50 50" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="25" cy="25" r="25"></circle><path d="M21.9163 13.5879L34.5459 35.4629H9.2868L21.9163 13.5879Z" fill="white"></path><path d="M26.7242 13.1627L42.7171 13.1627L34.7206 27.013L26.7242 13.1627Z" fill="white"></path></svg></div><div class="text-2xl font-semibold sm:block">philschmid</div></div></a></div><div class="flex items-center text-base leading-5"><div class="hidden md:block"><a class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2" href="https://www.philschmid.de/">Blog</a><a class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2" href="https://www.philschmid.de/cloud-attention">Newsletter</a><a class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2" href="https://www.philschmid.de/tags">Tags</a><a class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2" href="https://www.philschmid.de/projects">Projects</a><a class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2" href="https://www.philschmid.de/philipp-schmid">About Me</a><a target="_blank" rel="noopener noreferrer" href="mailto:schmidphilipp1995@gmail.com" class="p-1 font-medium text-gray-900 duration-100 hover:text-primary-500 dark:text-gray-100 dark:hover:text-primary-400 sm:p-2">Contact</a></div><button aria-label="Toggle Dark Mode" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 sm:ml-4"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><div class="md:hidden"><button type="button" class="ml-1 mr-1 h-8 w-8 rounded py-1" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="fixed top-0 left-0 z-10 h-full w-full transform bg-white opacity-95 duration-300 ease-in-out dark:bg-gray-800 translate-x-full"><div class="flex justify-end"><button type="button" class="mr-5 mt-11 h-8 w-8 rounded" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="https://www.philschmid.de/">Blog</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="https://www.philschmid.de/cloud-attention">Newsletter</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="https://www.philschmid.de/tags">Tags</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="https://www.philschmid.de/projects">Projects</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="https://www.philschmid.de/philipp-schmid">About Me</a></div><div class="px-12 py-4"><a target="_blank" rel="noopener noreferrer" href="mailto:schmidphilipp1995@gmail.com" class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100">Contact</a></div></nav></div></div></div></header><main class="mb-auto"><div class="mx-auto max-w-3xl px-1.5 sm:px-6 lg:max-w-4xl xl:max-w-6xl xl:px-0"><div class="fixed right-8 bottom-8 hidden flex-col gap-3 md:flex"><button aria-label="Scroll To Top" type="button" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div><header class="pt-6 xl:pb-6"><div class="space-y-1"><div><h1 class="font-serif text-3xl leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention</h1></div><div><div class="flex flex-wrap"><a class="mr-3 text-lg font-medium uppercase text-primary-600 hover:text-primary-500 dark:hover:text-primary-400" href="https://www.philschmid.de/tags/generativeai">#GenerativeAI</a><a class="mr-3 text-lg font-medium uppercase text-primary-600 hover:text-primary-500 dark:hover:text-primary-400" href="https://www.philschmid.de/tags/huggingface">#HuggingFace</a><a class="mr-3 text-lg font-medium uppercase text-primary-600 hover:text-primary-500 dark:hover:text-primary-400" href="https://www.philschmid.de/tags/llm">#LLM</a><a class="mr-3 text-lg font-medium uppercase text-primary-600 hover:text-primary-500 dark:hover:text-primary-400" href="https://www.philschmid.de/tags/deepspeed">#Deepspeed</a></div></div></div></header><div class="divide-y divide-gray-200 pb-8 dark:divide-gray-700 md:divide-y-0 lg:grid lg:grid-cols-4 xl:grid-cols-11 xl:gap-x-6" style="grid-template-rows:auto 1fr"><dl class="pt-6 pb-10 xl:col-span-2"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt><dd class="text-base leading-6 text-gray-500 dark:text-gray-400"><time datetime="2023-09-20T00:00:00.000Z">September 20, 2023</time></dd><dd class="text-base leading-6 text-gray-500 dark:text-gray-400">11 min read</dd><a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/deepseed-falcon-180b-lora-fa.ipynb" class="pt-6 pb-6 text-primary-500 hover:text-primary-600 dark:hover:text-primary-400">View Code</a></div></dl></dl><div class="divide-y divide-gray-200 dark:divide-gray-700 lg:col-span-3 xl:col-span-7 xl:col-start-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pt-6 pb-8 font-serif dark:prose-dark"><p>Falcon 180B is the newest version of Falcon LLM family. It is the biggest open source model with 180B parameter and trained on more data - 3.5T tokens with context length window upto 4K tokens. In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine.</p><p>In detail you will learn how to:</p><ol><li><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#1-setup-development-environment">Setup Development Environment</a></li><li><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#2-load-and-prepare-the-dataset">Load and prepare the dataset</a></li><li><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention">Fine-Tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention</a></li></ol><p>Before we get into the code lets take a quick look on the technologies and methods we are going to use:</p><h3 id="what-is-deepspeed-zero"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#what-is-deepspeed-zero" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>What is DeepSpeed ZeRO?</h3><p>DeepSpeed ZeRO focuses on efficient large-scale training of Transformers. ZeRO, or Zero Redundancy Optimizer, reduces memory footprint by partitioning model states across devices instead of basic data parallelism. This saves significant memory - ZeRO-Infinity can reduce usage 100x vs data parallelism. ZeRO-Offload further reduces memory by offloading parts of model and optimizer to CPU, enabling 10B+ parameter models on 1 GPU. ZeRO <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed">integrates with HuggingFace Transformers through a configuration file</a>.</p><h3 id="what-is-lora"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#what-is-lora" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>What is LoRA?</h3><p><a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2106.09685">LoRA</a> enables efficient fine-tuning of large language models. It decomposes weight matrices into smaller, trainable update matrices that adapt while keeping original weights frozen. This drastically reduces trainable parameters for faster, lower-memory tuning. LoRA integrates into <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/peft/conceptual_guides/lora">Transformers via Hugging Face's PEFT</a>. It combines well with methods like DeepSpeed. Key advantages are efficient tuning, portable models, and no inference latency when merging trained weights. LoRA allows adaptively training massive models with limited resources.</p><h3 id="what-is-flash-attention"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#what-is-flash-attention" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>What is Flash Attention?</h3><p>Flash Attention is an algorithm that speeds up the core attention mechanism in Transformer language models by restructuring computations. It uses techniques like tiling and recomputation to reduce the high memory costs of attention, enabling models to process longer text sequences. Flash Attention 2 optimizes parallelism and work partitioning for 2x speedup over the previous version, reaching 230 TFLOPS/s on A100 GPUs.</p><h3 id="access-falcon-180b"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#access-falcon-180b" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Access Falcon 180B</h3><p>Before we can start training we have to make sure that we accepted the license <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/tiiuae/falcon-180B">tiiuae/falcon-180B</a> to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at:</p><ul><li><a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/tiiuae/falcon-180B">tiiuae/falcon-180B</a></li></ul><blockquote><p>The example was created and run a DGX A100 8-GPU machine with 80GB GPU memory per GPU.</p></blockquote><h2 id="1-setup-development-environment"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#1-setup-development-environment" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>1. Setup Development Environment</h2><p>conda create --name hf python=3.10 -c conda-forge</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># install torch with the correct cuda version, check nvcc --version</span>
</span><span class="code-line">!pip install torch <span class="token operator">-</span><span class="token operator">-</span>extra<span class="token operator">-</span>index<span class="token operator">-</span>url https<span class="token punctuation">:</span><span class="token operator">//</span>download<span class="token punctuation">.</span>pytorch<span class="token punctuation">.</span>org<span class="token operator">/</span>whl<span class="token operator">/</span>cu118 <span class="token operator">-</span><span class="token operator">-</span>upgrade
</span><span class="code-line"><span class="token comment"># install Hugging Face Libraries and additional dependencies</span>
</span><span class="code-line">!pip install <span class="token string">"transformers==4.33.1"</span> <span class="token string">"datasets==2.14.5"</span> <span class="token string">"accelerate==0.22.0"</span> <span class="token string">"evaluate==0.4.0"</span> <span class="token string">"peft==0.5.0"</span> tensorboard packaging <span class="token operator">-</span><span class="token operator">-</span>upgrade
</span><span class="code-line"><span class="token comment"># install deepspeed and ninja for jit compilations of kernels</span>
</span><span class="code-line">!pip install <span class="token string">"deepspeed==0.10.3"</span> ninja <span class="token operator">-</span><span class="token operator">-</span>upgrade
</span><span class="code-line"><span class="token comment"># install additional Flash Attention</span>
</span><span class="code-line">!pip install flash<span class="token operator">-</span>attn <span class="token operator">-</span><span class="token operator">-</span>no<span class="token operator">-</span>build<span class="token operator">-</span>isolation <span class="token operator">-</span><span class="token operator">-</span>upgrade
</span></code></pre></div><p>To access any Falcon 180B asset we need to login into our hugging face account. We can do this by running the following command:</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">!huggingface<span class="token operator">-</span>cli login <span class="token operator">-</span><span class="token operator">-</span>token YOUR_TOKEN
</span></code></pre></div><h2 id="2-load-and-prepare-the-dataset"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#2-load-and-prepare-the-dataset" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>2. Load and prepare the dataset</h2><p>we will use the&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/datasets/databricks/databricks-dolly-15k">dolly</a>&nbsp;an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the&nbsp;<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2203.02155">InstructGPT paper</a>, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token punctuation">{</span>
</span><span class="code-line">  <span class="token string">"instruction"</span><span class="token punctuation">:</span> <span class="token string">"What is world of warcraft"</span><span class="token punctuation">,</span>
</span><span class="code-line">  <span class="token string">"context"</span><span class="token punctuation">:</span> <span class="token string">""</span><span class="token punctuation">,</span>
</span><span class="code-line">  <span class="token string">"response"</span><span class="token punctuation">:</span> <span class="token string">"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment"</span>
</span><span class="code-line"><span class="token punctuation">}</span>
</span></code></pre></div><p>To load the&nbsp;<code>samsum</code>&nbsp;dataset, we use the&nbsp;<code>load_dataset()</code>&nbsp;method from the ðŸ¤— Datasets library.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
</span><span class="code-line"><span class="token keyword">from</span> random <span class="token keyword">import</span> randrange
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># Load dataset from the hub</span>
</span><span class="code-line">dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"databricks/databricks-dolly-15k"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"dataset size: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span><span class="code-line"><span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>randrange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line"><span class="token comment"># dataset size: 15011</span>
</span></code></pre></div><p>To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a <code>formatting_function</code> that takes a sample and returns a string with our format instruction.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">def</span> <span class="token function">format_dolly</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    instruction <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"### Instruction\n</span><span class="token interpolation"><span class="token punctuation">{</span>sample<span class="token punctuation">[</span><span class="token string">'instruction'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span>
</span><span class="code-line">    context <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"### Context\n</span><span class="token interpolation"><span class="token punctuation">{</span>sample<span class="token punctuation">[</span><span class="token string">'context'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span> <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">"context"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
</span><span class="code-line">    response <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"### Answer\n</span><span class="token interpolation"><span class="token punctuation">{</span>sample<span class="token punctuation">[</span><span class="token string">'response'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span>
</span><span class="code-line">    <span class="token comment"># join all the parts together</span>
</span><span class="code-line">    prompt <span class="token operator">=</span> <span class="token string">"\n\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token punctuation">[</span>instruction<span class="token punctuation">,</span> context<span class="token punctuation">,</span> response<span class="token punctuation">]</span> <span class="token keyword">if</span> i <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token keyword">return</span> prompt
</span><span class="code-line">
</span></code></pre></div><p>lets test our formatting function on a random example.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">from</span> random <span class="token keyword">import</span> randrange
</span><span class="code-line">
</span><span class="code-line"><span class="token keyword">print</span><span class="token punctuation">(</span>format_dolly<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>randrange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</span></code></pre></div><p>In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer
</span><span class="code-line">
</span><span class="code-line">model_id <span class="token operator">=</span> <span class="token string">"tiiuae/falcon-180B"</span>
</span><span class="code-line">tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
</span><span class="code-line">tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token
</span></code></pre></div><p>We define some helper functions to pack our samples into sequences of a given length and then tokenize them.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">from</span> random <span class="token keyword">import</span> randint
</span><span class="code-line"><span class="token keyword">from</span> itertools <span class="token keyword">import</span> chain
</span><span class="code-line"><span class="token keyword">from</span> functools <span class="token keyword">import</span> partial
</span><span class="code-line">
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># template dataset to add prompt to each sample</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">template_dataset</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    sample<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>format_dolly<span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token interpolation"><span class="token punctuation">{</span>tokenizer<span class="token punctuation">.</span>eos_token<span class="token punctuation">}</span></span><span class="token string">"</span></span>
</span><span class="code-line">    <span class="token keyword">return</span> sample
</span><span class="code-line">
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># apply prompt template per sample</span>
</span><span class="code-line">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>template_dataset<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line"><span class="token comment"># print random sample</span>
</span><span class="code-line"><span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># empty list to save remainder from batches to use in next batch</span>
</span><span class="code-line">remainder <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"input_ids"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"attention_mask"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"token_type_ids"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">chunk</span><span class="token punctuation">(</span>sample<span class="token punctuation">,</span> chunk_length<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    <span class="token comment"># define global remainder variable to save remainder from batches to use in next batch</span>
</span><span class="code-line">    <span class="token keyword">global</span> remainder
</span><span class="code-line">    <span class="token comment"># Concatenate all texts and add remainder from previous batch</span>
</span><span class="code-line">    concatenated_examples <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">(</span>chain<span class="token punctuation">(</span><span class="token operator">*</span>sample<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> sample<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</span><span class="code-line">    concatenated_examples <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> remainder<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> concatenated_examples<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</span><span class="code-line">    <span class="token comment"># get total number of tokens for batch</span>
</span><span class="code-line">    batch_total_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>concatenated_examples<span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>sample<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line">    <span class="token comment"># get max number of chunks for batch</span>
</span><span class="code-line">    <span class="token keyword">if</span> batch_total_length <span class="token operator">&gt;=</span> chunk_length<span class="token punctuation">:</span>
</span><span class="code-line">        batch_chunk_length <span class="token operator">=</span> <span class="token punctuation">(</span>batch_total_length <span class="token operator">//</span> chunk_length<span class="token punctuation">)</span> <span class="token operator">*</span> chunk_length
</span><span class="code-line">
</span><span class="code-line">    <span class="token comment"># Split by chunks of max_len.</span>
</span><span class="code-line">    result <span class="token operator">=</span> <span class="token punctuation">{</span>
</span><span class="code-line">        k<span class="token punctuation">:</span> <span class="token punctuation">[</span>t<span class="token punctuation">[</span>i <span class="token punctuation">:</span> i <span class="token operator">+</span> chunk_length<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> batch_chunk_length<span class="token punctuation">,</span> chunk_length<span class="token punctuation">)</span><span class="token punctuation">]</span>
</span><span class="code-line">        <span class="token keyword">for</span> k<span class="token punctuation">,</span> t <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token punctuation">}</span>
</span><span class="code-line">    <span class="token comment"># add remainder to global variable for next batch</span>
</span><span class="code-line">    remainder <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> concatenated_examples<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">[</span>batch_chunk_length<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> concatenated_examples<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</span><span class="code-line">    <span class="token comment"># prepare labels</span>
</span><span class="code-line">    result<span class="token punctuation">[</span><span class="token string">"labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> result<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token keyword">return</span> result
</span><span class="code-line">
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># tokenize and chunk dataset</span>
</span><span class="code-line">lm_dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>
</span><span class="code-line">    <span class="token keyword">lambda</span> sample<span class="token punctuation">:</span> tokenizer<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>dataset<span class="token punctuation">.</span>features<span class="token punctuation">)</span>
</span><span class="code-line"><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>
</span><span class="code-line">    partial<span class="token punctuation">(</span>chunk<span class="token punctuation">,</span> chunk_length<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
</span><span class="code-line">    batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
</span><span class="code-line"><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># Print total number of samples</span>
</span><span class="code-line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Total number of samples: </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">len</span><span class="token punctuation">(</span>lm_dataset<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</span></code></pre></div><p>After we processed the datasets we want to save it to disk to be able to use the processed dataset later during training.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">lm_dataset<span class="token punctuation">.</span>save_to_disk<span class="token punctuation">(</span><span class="token string">"dolly-processed"</span><span class="token punctuation">)</span>
</span></code></pre></div><h2 id="3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>3. Fine-Tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention</h2><p>DeepSpeed ZeRO is natively integrated into the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/transformers/v4.33.1/en/main_classes/deepspeed">Hugging Face Transformers Trainer</a>. The integration enables leveraging ZeRO by simply providing a DeepSpeed config file, and the Trainer takes care of the rest. We created 2 deepspeed configurations for the experiments we ran, including <code>CPU offloading</code>:</p><ul><li><a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3.json">ds_falcon_180b_z3.json</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3_offload.json">ds_falcon_180b_z3_offload.json</a></li></ul><p>As mentioned in the beginning, we ran those example using a 8x NVIDIA A100 80GB. This means we can leverage <code>bf16</code>, which reduces the memory footprint of the model by almost ~2x, which allows us to train without offloading efficiently. We are going to use the <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3.json">ds_falcon_180b_z3.json</a>. If you are irritated by the <code>auto</code> values, check the <a target="_blank" rel="noopener noreferrer" href="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed#configuration">documentation</a>.</p><p>In addition to the deepspeed configuration we also need a training script, which implements LoRA and patches our model to use flash-attention. We created a <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/run_ds_lora.py">run_ds_lora.py</a> script, which patches the falcon model using the <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/utils/falcon_patch.py">falcon_patch.py</a> utils and implements LoRA using <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/utils/peft_utils.py">peft_utils.py</a>.</p><blockquote><p>When you run make sure that you have the same folder structure and utils/configs available. The easiest way is to clone the whole repository. Go into the <code>training</code> directory and start the training.</p></blockquote><p>Once we made sure that we have the right configuration and training script we can start the training using <code>torchrun</code>.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">!torchrun <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node <span class="token number">8</span> run_ds_lora<span class="token punctuation">.</span>py \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>model_id tiiuae<span class="token operator">/</span>falcon<span class="token operator">-</span>180B \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>dataset_path dolly<span class="token operator">-</span>processed \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>output_dir falcon<span class="token operator">-</span>180b<span class="token operator">-</span>lora<span class="token operator">-</span>fa \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>num_train_epochs <span class="token number">3</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>per_device_train_batch_size <span class="token number">1</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>learning_rate <span class="token number">4e-3</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>gradient_checkpointing <span class="token boolean">True</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>gradient_accumulation_steps <span class="token number">8</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>bf16 <span class="token boolean">True</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>tf32 <span class="token boolean">True</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>use_flash_attn <span class="token boolean">True</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>lr_scheduler_type <span class="token string">"constant_with_warmup"</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>logging_steps <span class="token number">25</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>save_steps <span class="token number">100</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>save_total_limit <span class="token number">3</span> \
</span><span class="code-line">  <span class="token operator">-</span><span class="token operator">-</span>deepspeed configs<span class="token operator">/</span>ds_falcon_180b_z3<span class="token punctuation">.</span>json
</span></code></pre></div><p><em>Note: Since we are using LoRA we are only saving the "trained" adapter weights, to save some storage. If you want to merge the adapters back into the base model and save the merged model you can add <code>--merge_adapters True</code> or use the <a target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/scripts/merge_adapter_weights.py">merge_adapter_weights.py</a> script.</em></p><p>In our example for Falcon 180B, the training time was <code>153 minutes</code> or ~2 hours for 3 epochs. For comparison the pretraining cost of Falcon 180B was ~7,000,000 GPU hours, which is 3,500,000 time more than fine-tuning.</p><h2 id="conclusion"><a href="https://www.philschmid.de/deepspeed-lora-flash-attention#conclusion" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Conclusion</h2><p>In the blog post you learn how to fine-tune Falcon 180B model using DeepSpeed, Hugging Face Transformers, and LoRA with Flash Attention on a multi-GPU machine. We used:</p><ul><li>DeepSpeed ZeRO for memory optimization, enabling training models with up to trillions of parameters on limited GPU memory. We used stage 3 (ZeRO-Infinity) to optimize memory usage.</li><li>Hugging Face Transformers and Datasets for easily loading and preparing the text dataset as well as providing an intuitive Trainer API.</li><li>LoRA, a method to efficiently fine-tune large language models by only updating a small percentage of parameters each iteration. This drastically reduces memory usage and computational costs.</li><li>Flash Attention - a highly optimized attention implementation that further reduces the memory footprint.</li></ul><p>Compining all of those methods allows us to fine-tune LLMs with over 100B+ parameter with limited resources. The example provides a template for efficiently tuning the largest publicly available models.</p><hr><p>Thanks for reading! If you have any questions, feel free to contact me on <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/_philschmid">Twitter</a> or <a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/philipp-schmid-a6a2bb196/">LinkedIn</a>.</p></div><div class="pt-6 pb-6 text-sm text-gray-700 dark:text-gray-300"><a target="_blank" rel="nofollow" href="https://mobile.twitter.com/search?q=https%3A%2F%2Fwww.philschmid.de%2Fdeepspeed-lora-flash-attention">Discuss on Twitter</a> â€¢ <a target="_blank" rel="noopener noreferrer" href="hhttps://github.com/philschmid/philschmid-de-v2/blob/master/data/blog/deepspeed-lora-flash-attention.mdx">View on GitHub</a></div></div></div></div></article></div></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-3 flex space-x-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:schmidphilipp1995@gmail.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M2.003 5.884 10 9.882l7.997-3.998A2 2 0 0 0 16 4H4a2 2 0 0 0-1.997 1.884z"></path><path d="m18 8.118-8 4-8-4V14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8.118z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/philschmid"><span class="sr-only">github</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/philipp-schmid-a6a2bb196/"><span class="sr-only">linkedin</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://twitter.com/_philschmid"><span class="sr-only">twitter</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M23.953 4.57a10 10 0 0 1-2.825.775 4.958 4.958 0 0 0 2.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 0 0-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 0 0-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 0 1-2.228-.616v.06a4.923 4.923 0 0 0 3.946 4.827 4.996 4.996 0 0 1-2.212.085 4.936 4.936 0 0 0 4.604 3.417 9.867 9.867 0 0 1-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 0 0 7.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0 0 24 4.59z"></path></svg></a></div><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>Philipp Schmid</div><div> â€¢ </div><div>Â© 2023</div><div> â€¢ </div><a href="https://www.philschmid.de/">philschmid blog</a><div> â€¢ </div><a href="https://www.philschmid.de/privacy-policy">Privacy Policy</a><div> â€¢ </div><a href="https://www.philschmid.de/imprint">Imprint</a></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"mdxSource":"var Component=(()=\u003e{var r=Object.create;var c=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var l=a=\u003ec(a,\"__esModule\",{value:!0});var u=(a,s)=\u003e()=\u003e(s||a((s={exports:{}}).exports,s),s.exports),N=(a,s)=\u003e{l(a);for(var t in s)c(a,t,{get:s[t],enumerable:!0})},k=(a,s,t)=\u003e{if(s\u0026\u0026typeof s==\"object\"||typeof s==\"function\")for(let n of d(s))!m.call(a,n)\u0026\u0026n!==\"default\"\u0026\u0026c(a,n,{get:()=\u003es[n],enumerable:!(t=p(s,n))||t.enumerable});return a},g=a=\u003ek(l(c(a!=null?r(h(a)):{},\"default\",a\u0026\u0026a.__esModule\u0026\u0026\"default\"in a?{get:()=\u003ea.default,enumerable:!0}:{value:a,enumerable:!0})),a);var o=u((v,i)=\u003e{i.exports=_jsx_runtime});var w={};N(w,{default:()=\u003eb,frontmatter:()=\u003ef});var e=g(o()),f={title:\"Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA \u0026 Flash Attention\",date:\"2023-09-20\",lastmod:\"2023-09-20\",tags:[\"GenerativeAI\",\"HuggingFace\",\"LLM\",\"Deepspeed\"],draft:!1,summary:\"In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine.\",images:[\"/static/blog/deepspeed-lora-flash-attention/thumbnail.jpg\"],repository:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/deepseed-falcon-180b-lora-fa.ipynb\"};function y(a={}){let{wrapper:s}=a.components||{};return s?(0,e.jsx)(s,Object.assign({},a,{children:(0,e.jsx)(t,{})})):t();function t(){let n=Object.assign({p:\"p\",ol:\"ol\",li:\"li\",a:\"a\",h3:\"h3\",span:\"span\",ul:\"ul\",blockquote:\"blockquote\",h2:\"h2\",pre:\"pre\",code:\"code\",em:\"em\",hr:\"hr\"},a.components);return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(n.p,{children:\"Falcon 180B is the newest version of Falcon LLM family. It is the biggest open source model with 180B parameter and trained on more data - 3.5T tokens with context length window upto 4K tokens. In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine.\"}),(0,e.jsx)(n.p,{children:\"In detail you will learn how to:\"}),(0,e.jsxs)(n.ol,{children:[(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#1-setup-development-environment\",children:\"Setup Development Environment\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#2-load-and-prepare-the-dataset\",children:\"Load and prepare the dataset\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"#3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention\",children:\"Fine-Tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention\"})})]}),(0,e.jsx)(n.p,{children:\"Before we get into the code lets take a quick look on the technologies and methods we are going to use:\"}),(0,e.jsxs)(n.h3,{id:\"what-is-deepspeed-zero\",children:[(0,e.jsx)(n.a,{href:\"#what-is-deepspeed-zero\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"What is DeepSpeed ZeRO?\"]}),(0,e.jsxs)(n.p,{children:[\"DeepSpeed ZeRO focuses on efficient large-scale training of Transformers. ZeRO, or Zero Redundancy Optimizer, reduces memory footprint by partitioning model states across devices instead of basic data parallelism. This saves significant memory - ZeRO-Infinity can reduce usage 100x vs data parallelism. ZeRO-Offload further reduces memory by offloading parts of model and optimizer to CPU, enabling 10B+ parameter models on 1 GPU. ZeRO \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed\",children:\"integrates with HuggingFace Transformers through a configuration file\"}),\".\"]}),(0,e.jsxs)(n.h3,{id:\"what-is-lora\",children:[(0,e.jsx)(n.a,{href:\"#what-is-lora\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"What is LoRA?\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.a,{href:\"https://arxiv.org/abs/2106.09685\",children:\"LoRA\"}),\" enables efficient fine-tuning of large language models. It decomposes weight matrices into smaller, trainable update matrices that adapt while keeping original weights frozen. This drastically reduces trainable parameters for faster, lower-memory tuning. LoRA integrates into \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/docs/peft/conceptual_guides/lora\",children:\"Transformers via Hugging Face's PEFT\"}),\". It combines well with methods like DeepSpeed. Key advantages are efficient tuning, portable models, and no inference latency when merging trained weights. LoRA allows adaptively training massive models with limited resources.\"]}),(0,e.jsxs)(n.h3,{id:\"what-is-flash-attention\",children:[(0,e.jsx)(n.a,{href:\"#what-is-flash-attention\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"What is Flash Attention?\"]}),(0,e.jsx)(n.p,{children:\"Flash Attention is an algorithm that speeds up the core attention mechanism in Transformer language models by restructuring computations. It uses techniques like tiling and recomputation to reduce the high memory costs of attention, enabling models to process longer text sequences. Flash Attention 2 optimizes parallelism and work partitioning for 2x speedup over the previous version, reaching 230 TFLOPS/s on A100 GPUs.\"}),(0,e.jsxs)(n.h3,{id:\"access-falcon-180b\",children:[(0,e.jsx)(n.a,{href:\"#access-falcon-180b\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Access Falcon 180B\"]}),(0,e.jsxs)(n.p,{children:[\"Before we can start training we have to make sure that we accepted the license \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/tiiuae/falcon-180B\",children:\"tiiuae/falcon-180B\"}),\" to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at:\"]}),(0,e.jsx)(n.ul,{children:(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://huggingface.co/tiiuae/falcon-180B\",children:\"tiiuae/falcon-180B\"})})}),(0,e.jsx)(n.blockquote,{children:(0,e.jsx)(n.p,{children:\"The example was created and run a DGX A100 8-GPU machine with 80GB GPU memory per GPU.\"})}),(0,e.jsxs)(n.h2,{id:\"1-setup-development-environment\",children:[(0,e.jsx)(n.a,{href:\"#1-setup-development-environment\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"1. Setup Development Environment\"]}),(0,e.jsx)(n.p,{children:\"conda create --name hf python=3.10 -c conda-forge\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# install torch with the correct cuda version, check nvcc --version\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!pip install torch \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"extra\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"index\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"url https\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\"download\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"pytorch\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"org\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"whl\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"cu118 \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`upgrade\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# install Hugging Face Libraries and additional dependencies\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!pip install \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"transformers==4.33.1\"'}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"datasets==2.14.5\"'}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"accelerate==0.22.0\"'}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"evaluate==0.4.0\"'}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"peft==0.5.0\"'}),\" tensorboard packaging \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`upgrade\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# install deepspeed and ninja for jit compilations of kernels\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!pip install \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"deepspeed==0.10.3\"'}),\" ninja \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`upgrade\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# install additional Flash Attention\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!pip install flash\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"attn \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"no\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"build\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"isolation \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`upgrade\n`]})]})}),(0,e.jsx)(n.p,{children:\"To access any Falcon 180B asset we need to login into our hugging face account. We can do this by running the following command:\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!huggingface\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"cli login \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`token YOUR_TOKEN\n`]})})}),(0,e.jsxs)(n.h2,{id:\"2-load-and-prepare-the-dataset\",children:[(0,e.jsx)(n.a,{href:\"#2-load-and-prepare-the-dataset\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"2. Load and prepare the dataset\"]}),(0,e.jsxs)(n.p,{children:[\"we will use the\\xA0\",(0,e.jsx)(n.a,{href:\"https://huggingface.co/datasets/databricks/databricks-dolly-15k\",children:\"dolly\"}),\"\\xA0an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the\\xA0\",(0,e.jsx)(n.a,{href:\"https://arxiv.org/abs/2203.02155\",children:\"InstructGPT paper\"}),\", including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\"]}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"instruction\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"What is world of warcraft\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"context\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"response\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"'}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]})]})}),(0,e.jsxs)(n.p,{children:[\"To load the\\xA0\",(0,e.jsx)(n.code,{children:\"samsum\"}),\"\\xA0dataset, we use the\\xA0\",(0,e.jsx)(n.code,{children:\"load_dataset()\"}),\"\\xA0method from the \\u{1F917} Datasets library.\"]}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" datasets \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` load_dataset\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" random \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` randrange\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# Load dataset from the hub\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dataset \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" load_dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"databricks/databricks-dolly-15k\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" split\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"train\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"dataset size: '}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"randrange\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# dataset size: 15011\"}),`\n`]})]})}),(0,e.jsxs)(n.p,{children:[\"To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a \",(0,e.jsx)(n.code,{children:\"formatting_function\"}),\" that takes a sample and returns a string with our format instruction.\"]}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"format_dolly\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    instruction \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"### Instruction\\\\n'}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'instruction'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    context \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"### Context\\\\n'}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'context'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"if\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"context\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"\u003e\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"else\"}),\" \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    response \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"### Answer\\\\n'}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'response'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# join all the parts together\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    prompt \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"\\\\n\\\\n\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"join\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"i \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" i \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"instruction\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" context\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" response\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"if\"}),\" i \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"is\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"not\"}),\" \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),` prompt\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`})]})}),(0,e.jsx)(n.p,{children:\"lets test our formatting function on a random example.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" random \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` randrange\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"format_dolly\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"randrange\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:\"In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" transformers \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` AutoTokenizer\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"model_id \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"tiiuae/falcon-180B\"'}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"tokenizer \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" AutoTokenizer\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"from_pretrained\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"model_id\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"tokenizer\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"pad_token \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" tokenizer\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`eos_token\n`]})]})}),(0,e.jsx)(n.p,{children:\"We define some helper functions to pack our samples into sequences of a given length and then tokenize them.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" random \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` randint\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" itertools \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` chain\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" functools \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),` partial\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# template dataset to add prompt to each sample\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"template_dataset\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"text\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"'}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"format_dolly\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"tokenizer\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"eos_token\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),` sample\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# apply prompt template per sample\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dataset \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"map\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"template_dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" remove_columns\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"list\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"features\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# print random sample\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"randint\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"text\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# empty list to save remainder from batches to use in next batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"remainder \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"input_ids\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"attention_mask\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"token_type_ids\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"chunk\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2048\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# define global remainder variable to save remainder from batches to use in next batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"global\"}),` remainder\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# Concatenate all texts and add remainder from previous batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    concatenated_examples \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"list\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"chain\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" k \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"keys\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    concatenated_examples \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" remainder\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"+\"}),\" concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" k \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"keys\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# get total number of tokens for batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    batch_total_length \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"list\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"keys\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# get max number of chunks for batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"if\"}),\" batch_total_length \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"\u003e=\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        batch_chunk_length \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"batch_total_length \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),` chunk_length\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# Split by chunks of max_len.\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    result \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"t\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"i \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" i \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"+\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" i \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"range\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" batch_chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" t \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"items\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# add remainder to global variable for next batch\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    remainder \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"batch_chunk_length\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" k \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" concatenated_examples\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"keys\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# prepare labels\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    result\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"labels\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" result\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"input_ids\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"copy\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),` result\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# tokenize and chunk dataset\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"lm_dataset \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"map\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"lambda\"}),\" sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" tokenizer\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"sample\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"text\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" batched\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" remove_columns\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"list\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"features\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"map\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    partial\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"chunk\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" chunk_length\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2048\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    batched\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# Print total number of samples\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"print\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:'f\"Total number of samples: '}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"lm_dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"'})]}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:\"After we processed the datasets we want to save it to disk to be able to use the processed dataset later during training.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"lm_dataset\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"save_to_disk\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token string\",children:'\"dolly-processed\"'}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsxs)(n.h2,{id:\"3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention\",children:[(0,e.jsx)(n.a,{href:\"#3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"3. Fine-Tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention\"]}),(0,e.jsxs)(n.p,{children:[\"DeepSpeed ZeRO is natively integrated into the \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/docs/transformers/v4.33.1/en/main_classes/deepspeed\",children:\"Hugging Face Transformers Trainer\"}),\". The integration enables leveraging ZeRO by simply providing a DeepSpeed config file, and the Trainer takes care of the rest. We created 2 deepspeed configurations for the experiments we ran, including \",(0,e.jsx)(n.code,{children:\"CPU offloading\"}),\":\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3.json\",children:\"ds_falcon_180b_z3.json\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3_offload.json\",children:\"ds_falcon_180b_z3_offload.json\"})})]}),(0,e.jsxs)(n.p,{children:[\"As mentioned in the beginning, we ran those example using a 8x NVIDIA A100 80GB. This means we can leverage \",(0,e.jsx)(n.code,{children:\"bf16\"}),\", which reduces the memory footprint of the model by almost ~2x, which allows us to train without offloading efficiently. We are going to use the \",(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/configs/ds_falcon_180b_z3.json\",children:\"ds_falcon_180b_z3.json\"}),\". If you are irritated by the \",(0,e.jsx)(n.code,{children:\"auto\"}),\" values, check the \",(0,e.jsx)(n.a,{href:\"https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed#configuration\",children:\"documentation\"}),\".\"]}),(0,e.jsxs)(n.p,{children:[\"In addition to the deepspeed configuration we also need a training script, which implements LoRA and patches our model to use flash-attention. We created a \",(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/run_ds_lora.py\",children:\"run_ds_lora.py\"}),\" script, which patches the falcon model using the \",(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/utils/falcon_patch.py\",children:\"falcon_patch.py\"}),\" utils and implements LoRA using \",(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/utils/peft_utils.py\",children:\"peft_utils.py\"}),\".\"]}),(0,e.jsx)(n.blockquote,{children:(0,e.jsxs)(n.p,{children:[\"When you run make sure that you have the same folder structure and utils/configs available. The easiest way is to clone the whole repository. Go into the \",(0,e.jsx)(n.code,{children:\"training\"}),\" directory and start the training.\"]})}),(0,e.jsxs)(n.p,{children:[\"Once we made sure that we have the right configuration and training script we can start the training using \",(0,e.jsx)(n.code,{children:\"torchrun\"}),\".\"]}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"!torchrun \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"nproc_per_node \",(0,e.jsx)(n.span,{className:\"token number\",children:\"8\"}),\" run_ds_lora\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`py \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"model_id tiiuae\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"falcon\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`180B \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"dataset_path dolly\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`processed \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"output_dir falcon\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"180b\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"lora\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),`fa \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"num_train_epochs \",(0,e.jsx)(n.span,{className:\"token number\",children:\"3\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"per_device_train_batch_size \",(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"learning_rate \",(0,e.jsx)(n.span,{className:\"token number\",children:\"4e-3\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"gradient_checkpointing \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"gradient_accumulation_steps \",(0,e.jsx)(n.span,{className:\"token number\",children:\"8\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"bf16 \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"tf32 \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"use_flash_attn \",(0,e.jsx)(n.span,{className:\"token boolean\",children:\"True\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"lr_scheduler_type \",(0,e.jsx)(n.span,{className:\"token string\",children:'\"constant_with_warmup\"'}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"logging_steps \",(0,e.jsx)(n.span,{className:\"token number\",children:\"25\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"save_steps \",(0,e.jsx)(n.span,{className:\"token number\",children:\"100\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"save_total_limit \",(0,e.jsx)(n.span,{className:\"token number\",children:\"3\"}),` \\\\\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"deepspeed configs\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"ds_falcon_180b_z3\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`json\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsxs)(n.em,{children:['Note: Since we are using LoRA we are only saving the \"trained\" adapter weights, to save some storage. If you want to merge the adapters back into the base model and save the merged model you can add ',(0,e.jsx)(n.code,{children:\"--merge_adapters True\"}),\" or use the \",(0,e.jsx)(n.a,{href:\"https://github.com/philschmid/deep-learning-pytorch-huggingface/tree/main/training/scripts/merge_adapter_weights.py\",children:\"merge_adapter_weights.py\"}),\" script.\"]})}),(0,e.jsxs)(n.p,{children:[\"In our example for Falcon 180B, the training time was \",(0,e.jsx)(n.code,{children:\"153 minutes\"}),\" or ~2 hours for 3 epochs. For comparison the pretraining cost of Falcon 180B was ~7,000,000 GPU hours, which is 3,500,000 time more than fine-tuning.\"]}),(0,e.jsxs)(n.h2,{id:\"conclusion\",children:[(0,e.jsx)(n.a,{href:\"#conclusion\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),(0,e.jsx)(n.p,{children:\"In the blog post you learn how to fine-tune Falcon 180B model using DeepSpeed, Hugging Face Transformers, and LoRA with Flash Attention on a multi-GPU machine. We used:\"}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"DeepSpeed ZeRO for memory optimization, enabling training models with up to trillions of parameters on limited GPU memory. We used stage 3 (ZeRO-Infinity) to optimize memory usage.\"}),(0,e.jsx)(n.li,{children:\"Hugging Face Transformers and Datasets for easily loading and preparing the text dataset as well as providing an intuitive Trainer API.\"}),(0,e.jsx)(n.li,{children:\"LoRA, a method to efficiently fine-tune large language models by only updating a small percentage of parameters each iteration. This drastically reduces memory usage and computational costs.\"}),(0,e.jsx)(n.li,{children:\"Flash Attention - a highly optimized attention implementation that further reduces the memory footprint.\"})]}),(0,e.jsx)(n.p,{children:\"Compining all of those methods allows us to fine-tune LLMs with over 100B+ parameter with limited resources. The example provides a template for efficiently tuning the largest publicly available models.\"}),(0,e.jsx)(n.hr,{}),(0,e.jsxs)(n.p,{children:[\"Thanks for reading! If you have any questions, feel free to contact me on \",(0,e.jsx)(n.a,{href:\"https://twitter.com/_philschmid\",children:\"Twitter\"}),\" or \",(0,e.jsx)(n.a,{href:\"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/\",children:\"LinkedIn\"}),\".\"]})]})}}var b=y;return w;})();\n;return Component;","toc":[{"value":"What is DeepSpeed ZeRO?","url":"#what-is-deepspeed-zero","depth":3},{"value":"What is LoRA?","url":"#what-is-lora","depth":3},{"value":"What is Flash Attention?","url":"#what-is-flash-attention","depth":3},{"value":"Access Falcon 180B","url":"#access-falcon-180b","depth":3},{"value":"1. Setup Development Environment","url":"#1-setup-development-environment","depth":2},{"value":"2. Load and prepare the dataset","url":"#2-load-and-prepare-the-dataset","depth":2},{"value":"3. Fine-Tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention","url":"#3-fine-tune-falcon-180b-using-deepspeed-hugging-face-transformers-lora-with-flash-attention","depth":2},{"value":"Conclusion","url":"#conclusion","depth":2}],"frontMatter":{"readingTime":{"text":"11 min read","minutes":10.38,"time":622800,"words":2076},"slug":"deepspeed-lora-flash-attention","fileName":"deepspeed-lora-flash-attention.mdx","title":"Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA \u0026 Flash Attention","date":"2023-09-20T00:00:00.000Z","lastmod":"2023-09-20","tags":["GenerativeAI","HuggingFace","LLM","Deepspeed"],"draft":false,"summary":"In this example we will show how to fine-tune Falcon 180B using DeepSpeed, Hugging Face Transformers, LoRA with Flash Attention on a multi-GPU machine.","images":["/static/blog/deepspeed-lora-flash-attention/thumbnail.jpg"],"repository":"https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/deepseed-falcon-180b-lora-fa.ipynb"}},"authorDetails":[{"readingTime":{"text":"2 min read","minutes":1.545,"time":92700,"words":309},"slug":["default"],"fileName":"default.md","name":"Philipp Schmid","avatar":"/static/images/author/philippschmid-medium.png","tags":["AWS","Huggingface","Pytorch","Azure","Serverless","NLP"],"location":"ðŸ“ Nuremberg, ðŸ‡©ðŸ‡ª Germany","occupation":"ðŸ§‘ðŸ»â€ðŸ’» Technical Lead","company":"ðŸ¤— Hugging Face","extra_1":"ðŸ¦¸ðŸ»â€â™‚ï¸ AWS ML Hero","email":"schmidphilipp1995@gmail.com","twitter":"https://twitter.com/_philschmid","linkedin":"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/","github":"https://github.com/philschmid","date":null}],"prev":{"title":"Fine-tune Falcon 180B with QLoRA and Flash Attention on Amazon SageMaker","date":"2023-09-12T00:00:00.000Z","lastmod":"2023-09-12","tags":["GenerativeAI","HuggingFace","LLM","SageMaker"],"draft":false,"summary":"Learn how to fine-tune Falcon 180B with QLoRA and Flash Attention on Amazon SageMaker.","images":["/static/blog/sagemaker-falcon-180b-qlora/thumbnail.jpg"],"repository":"https://github.com/philschmid/sagemaker-falcon-180b-samples/blob/master/training/sagemaker-notebook.ipynb","slug":"sagemaker-falcon-180b-qlora"},"next":null},"__N_SSG":true},"page":"/[...slug]","query":{"slug":["deepspeed-lora-flash-attention"]},"buildId":"g-7BcDxEJckFkizaqdO-_","isFallback":false,"gsp":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;"></p></next-route-announcer><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/js" data-nscript="lazyOnload"></script><script id="ga-script" data-nscript="lazyOnload">
          // https://developers.google.com/tag-manager/devguide
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());

          // defines window.localstorage key
          const GA_LOCAL_STORAGE_KEY = 'ga:clientId';

          // checks if localstorage is available
          if (window.localStorage) {
            // checks if user was already connected and loads client_id from localstorage
            if (localStorage.getItem(GA_LOCAL_STORAGE_KEY)) {
              // creates new tracker with same client_id as the last time the user visited
              gtag('js', new Date());
              gtag('config', 'UA-154999168-1', {
                page_path: window.location.pathname,
                client_storage: 'none',
                client_id: localStorage.getItem(GA_LOCAL_STORAGE_KEY),
              });
            } else {
              // creates client_id and saves it in localStorage -> currently random number better would be a uuid
              window.localStorage.setItem(GA_LOCAL_STORAGE_KEY, Math.random().toString(36).substr(2, 10) + new Date().getTime());
              // creates new tracker with the new client_id
              gtag('js', new Date());
              gtag('config', 'UA-154999168-1', {
                page_path: window.location.pathname,
                client_storage: 'none',
                client_id: localStorage.getItem(GA_LOCAL_STORAGE_KEY),
              });
            }
          }
        </script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/[tag]-2deb40458c121ea8.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/philipp-schmid-5fe320197331417d.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/projects-6b079a3b27a4ee68.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/index-6353731beeb76ac8.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/cloud-attention-1239786830d628f3.js.tÃ©lÃ©chargement"></script><script src="./Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA &amp; Flash Attention_files/tags-6899b210840c7080.js.tÃ©lÃ©chargement"></script></body></html>